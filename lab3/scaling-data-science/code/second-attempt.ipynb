{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32836ccc-8cf4-4a5a-998d-8cd6eee35a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd   # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import xgboost as xgb\n",
    "\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import ray\n",
    "\n",
    "# init ray and attach it to local node ray instance\n",
    "ray.init(address='auto')\n",
    "\n",
    "# function to perform the tuning using tune-search library\n",
    "# add function decorator\n",
    "@ray.remote\n",
    "def tune_search_tuning():\n",
    "\n",
    "    # Input data files are available in the \"/var/data/\" directory.\n",
    "    train_df = pd.read_csv(\"/home/ubuntu/de2/lab3/train.csv\")\n",
    "    dataset_size = 1000\n",
    "    train_df = train_df.iloc[0:dataset_size, :]\n",
    "    \n",
    "    y = train_df.label.values\n",
    "    x = train_df.drop('label', axis=1).values\n",
    "\n",
    "    # define the train set and test set\n",
    "    # in principle the test (valid) data is not used later, \n",
    "    # so we minimize the size to just 5%.\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.05)\n",
    "    print(\"Shapes - X_train: \", x_train.shape, \", X_val: \", x_val.shape, \", y_train: \", y_train.shape, \", y_val: \", y_val.shape)\n",
    "\n",
    "    # numpy arrays are not accepted in params attributes, \n",
    "    # so we use python comprehension notation to build lists\n",
    "    params = {'max_depth': [3, 6, 10, 15],\n",
    "              'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n",
    "              'subsample': [0.5 + x / 100 for x in range(10, 50, 10)],\n",
    "              'colsample_bytree': [0.5 + x / 100 for x in range(10, 50, 10)],\n",
    "              'colsample_bylevel': [0.5 + x / 100 for x in range(10, 50, 10)],\n",
    "              'n_estimators': [100, 500, 1000],\n",
    "              'num_class': [10]\n",
    "              }\n",
    "\n",
    "    # define the booster classifier indicating the objective as \n",
    "    # multiclass \"multi:softmax\" and try to speed up execution\n",
    "    # by setting parameter tree_method = \"hist\"\n",
    "    xgbclf = xgb.XGBClassifier(objective=\"multi:softmax\",\n",
    "                               tree_method=\"hist\")\n",
    "\n",
    "    # replace RamdomizedSearchCV by TuneSearchCV\n",
    "    # n_trials sets the number of iterations (different hyperparameter combinations)\n",
    "    # that will be evaluated\n",
    "    # verbosity can be set from 0 to 3 (debug level).\n",
    "    tune_search = TuneSearchCV(estimator=xgbclf,\n",
    "                               param_distributions=params,\n",
    "                               scoring='accuracy',\n",
    "                               n_trials=25,\n",
    "                               verbose=1)\n",
    "\n",
    "    # perform hyperparameter tuning\n",
    "    tune_search.fit(x_train, y_train)\n",
    "\n",
    "    print(\"cv results: \", tune_search.cv_results_)\n",
    "\n",
    "    best_combination = tune_search.best_params_\n",
    "    print(\"Best parameters:\", best_combination)\n",
    "\n",
    "    # evaluate accuracy based on the test dataset\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # create the task\n",
    "    remote_clf = tune_search_tuning.remote()\n",
    "\n",
    "    # get the task result\n",
    "    best_params = ray.get(remote_clf)\n",
    "\n",
    "    stop_time = time.time()\n",
    "    print(\"Stopping at :\", stop_time)\n",
    "    print(\"Total elapsed time: \", stop_time - start_time)\n",
    "\n",
    "    print(\"Best params from main function: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bbde6-d3c0-4adc-b553-534a9e854351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +33m46s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +33m46s)\u001b[0m Adding 1 nodes of type local.cluster.node.\n",
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +34m1s)\u001b[0m Resized to 4 CPUs.\n",
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +50m18s)\u001b[0m Removing 1 nodes of type local.cluster.node (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +50m28s)\u001b[0m Resized to 3 CPUs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd   # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import xgboost as xgb\n",
    "\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# function to perform the tuning using tune-search library\n",
    "def tune_search_tuning():\n",
    "\n",
    "    # Input data files are available in the \"./data/\" directory.\n",
    "    train_df = pd.read_csv(\"/home/ubuntu/train.csv\")\n",
    "    test_df = pd.read_csv(\"/home/ubuntu/test.csv\")\n",
    "\n",
    "    # limit dataset size to 1000 samples\n",
    "    dataset_size = 1000\n",
    "    train_df = train_df.iloc[0:dataset_size, :]\n",
    "    test_df = test_df.iloc[0:dataset_size, :]\n",
    "\n",
    "    print(\"Reduced dataset size: \", train_df.shape)\n",
    "\n",
    "    y_train = train_df.label.values\n",
    "    x_train = train_df.drop('label', axis=1).values\n",
    "\n",
    "    y_test = test_df.label.values\n",
    "    x_test = test_df.drop('label', axis=1).values\n",
    "\n",
    "    params = {'max_depth': [6, 10],\n",
    "              'learning_rate': [0.1, 0.3, 0.4],\n",
    "              'subsample': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "              'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "              'colsample_bylevel': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "              'n_estimators': [500, 1000],\n",
    "              'num_class': [10]\n",
    "              }\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"starting at: \", start_time)\n",
    "\n",
    "    # define the booster classifier indicating the objective\n",
    "    # as multiclass \"multi:softmax\" and try to speed up execution\n",
    "    # by setting parameter tree_method = \"hist\"\n",
    "    xgbclf = xgb.XGBClassifier(objective=\"multi:softmax\",\n",
    "                               tree_method=\"hist\")\n",
    "\n",
    "    # replace RamdomizedSearchCV by TuneSearchCV\n",
    "    # n_trials sets the number of iterations (different hyperparameter combinations)\n",
    "    # that will be evaluated\n",
    "\n",
    "    # verbosity can be set from 0 to 3 (debug level).\n",
    "    tune_search = TuneSearchCV(estimator=xgbclf,\n",
    "                               param_distributions=params,\n",
    "                               scoring='accuracy',\n",
    "                               n_trials=20,\n",
    "                               n_jobs=8,\n",
    "                               verbose=2)\n",
    "\n",
    "    # perform hyperparameter tuning\n",
    "    tune_search.fit(x_train, y_train)\n",
    "\n",
    "    stop_time = time.time()\n",
    "    print(\"Stopping at :\", stop_time)\n",
    "    print(\"Total elapsed time: \", stop_time - start_time)\n",
    "\n",
    "    best_combination = tune_search.best_params_\n",
    "\n",
    "    # evaluate accuracy based on the test dataset\n",
    "    predictions = tune_search.predict(x_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    best_params = tune_search_tuning()\n",
    "    print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b21f4b-b11b-4a7e-a26e-7d8d8fbfde07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272bad5-2da4-44d6-af23-f3acef514262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "import time\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import statistics\n",
    "import pandas as pd\n",
    "ray.init(address=\"auto\")\n",
    "\n",
    "def train(config):\n",
    "    trainDF = pd.read_csv(\"/home/ubuntu/test.csv\")\n",
    "    trainFeatures = trainDF.iloc[:, :-1] # all rows, all but last column\n",
    "    trainClasses = trainDF.iloc[:, -1] # all rows, only last column\n",
    "\n",
    "    RFclassifier = RandomForestClassifier()\n",
    "    params = RFclassifier.get_params()\n",
    "    sklearn.model_selection.cross_validate(RFclassifier, trainFeatures, trainClasses)\n",
    "    #print (train_df.shape, test_df.shape)\n",
    "    RFclassifier = RandomForestClassifier(**config)\n",
    "    params = RFclassifier.get_params()\n",
    "    xval = sklearn.model_selection.cross_validate(RFclassifier, trainFeatures, trainClasses)\n",
    "    #tune.report(mean_accuracy=statistics.mean(xval[\"test_score\"]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search_space = {\n",
    "    \"max_depth\": tune.grid_search([5*i for i in range(1,10)]),\n",
    "    \"n_estimators\": tune.grid_search([10,100,200,300]),\n",
    "    \"ccp_alpha\": tune.grid_search([0.0,0.1,0.2]),}\n",
    "    start_time = time.time()\n",
    "\n",
    "    # create the task\n",
    "    remote_clf = train(search_space)\n",
    "\n",
    "    # get the task result\n",
    "    #best_params = ray.get(remote_clf)\n",
    "\n",
    "    stop_time = time.time()\n",
    "    print(\"Stopping at :\", stop_time)\n",
    "    print(\"Total elapsed time: \", stop_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739826b6-1ddc-40d4-865d-b1554083a6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
